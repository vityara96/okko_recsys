{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField, StringType\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/test_users.json\"\n",
    "ratings_path = \"../data/base_rating_prq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName(\"ALSbuilder\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = spark.read.parquet(ratings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------+-------------------+\n",
      "|element_uid|user_uid|                ts|    true_watch_part|\n",
      "+-----------+--------+------------------+-------------------+\n",
      "|       7642|  398055| 42890955.82399276| 1.1703333333333332|\n",
      "|        775|  312530|42948120.417987145| 0.3293333333333333|\n",
      "|       9742|  459840| 44202466.49127187|0.06555555555555556|\n",
      "|       2694|  366950| 42078259.94467698| 1.1810833333333333|\n",
      "|      10061|  460080| 42566616.78668126| 0.9904999999999999|\n",
      "|       6432|  280809| 42103269.00310552| 0.8261666666666666|\n",
      "|        283|  117180| 42471787.11508496| 0.8853333333333333|\n",
      "|       8863|  433042| 43128230.36225684|   1.16929012345679|\n",
      "|       6728|  524287| 43893810.59373309|             0.0075|\n",
      "|       2024|  495586| 43821190.91957284|  1.170534188034188|\n",
      "|       6872|   95978|42310750.323095135|            1.20625|\n",
      "|       7150|  300940| 42638277.56487383| 1.1723484848484849|\n",
      "|       2567|  143250|  41766370.9448332| 0.4873809523809524|\n",
      "|       1130|  115928|  43054153.9377158| 1.1689166666666666|\n",
      "|       3230|  372542| 43489017.53329274|              0.961|\n",
      "|       4217|  278189|43292963.966486245| 0.8848333333333334|\n",
      "|       7449|  276227| 43030810.93822509|             0.9875|\n",
      "|       4607|   38109| 43269598.74901126|0.22152777777777777|\n",
      "|       8771|  267677| 42370939.37692031|0.07206031325488974|\n",
      "|       2360|  174335|  42120107.3831604| 0.9648148148148148|\n",
      "+-----------+--------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Заменяем имена колонок на стандарные и корректируем типы данных\n",
    "\n",
    "userCol = 'user_uid'\n",
    "itemCol = 'element_uid'\n",
    "ratingCol = 'true_watch_part'\n",
    "\n",
    "\n",
    "rating_df = rating_df.withColumnRenamed(itemCol, 'item_id')\\\n",
    "        .withColumnRenamed(ratingCol, 'rate')\\\n",
    "        .withColumnRenamed(userCol, 'user_id')\\\n",
    "        .withColumn(\"user_id\", col(\"user_id\").cast('int'))\\\n",
    "        .withColumn(\"item_id\", col(\"item_id\").cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Убираем пользователей, которые посмотрели меньше 3х фильмов\n",
    "\n",
    "film_cnt = rating_df.groupBy('user_id').count()\\\n",
    "            .withColumn('enough_films', col('count') >= 3)\n",
    "\n",
    "rating_df = rating_df.join(film_cnt, on='user_id', how='left')\\\n",
    "            .where(col('enough_films') == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOT split\n",
    "\n",
    "OOT разбиение на train и test (сделать ячейку активной, если необходимо использовать его)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def time_series_split(df, time_col, train_size=0.5):\n",
    "    \n",
    "    df = df.orderBy(time_col, ascending=True)\n",
    "    df_size = df.count()\n",
    "    train_size = int(df_size * train_size)\n",
    "    test_size = df_size - train_size \n",
    "    print(train_size, test_size)\n",
    "    train = df.limit(train_size)\n",
    "    df = df.orderBy(time_col, ascending=False)\n",
    "    test = df.limit(test_size)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_als, train_cb = time_series_split(rating_df, 'ts')\n",
    "\n",
    "train_cb.show()\n",
    "\n",
    "train_cb, test = time_series_split(train_cb, 'ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomSplit \n",
    "\n",
    "Случайное разбиение на train и test (сделать ячейку активной, если необходимо использовать его)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.drop(\"ts\").limit(10000)\n",
    "\n",
    "train_als, train_cb, test = rating_df.randomSplit([0.4, 0.4, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client oriented split on train/train/test\n",
    "\n",
    "Разбиение на train и test oot по каждому клиенту (сделать ячейку активной, если необходимо использовать его)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fin_cols = ['user_id', 'item_id', 'ts', 'rate']\n",
    "\n",
    "rating_df = rating_df.select(fin_cols)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_test_split(df, test_size=0.2):\n",
    "    \n",
    "    cols = df.columns\n",
    "    \n",
    "    window = Window.orderBy(\"ts\")\\\n",
    "                .partitionBy('user_id')\\\n",
    "                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    df = df.withColumn('group_index', row_number().over(window))\n",
    "    \n",
    "    group_sizes = rating_df.groupBy('user_id').count()\\\n",
    "                    .select('user_id', col('count').alias('group_size'))\\\n",
    "                    .withColumn('test', ceil(col('group_size') * test_size))\\\n",
    "                    .withColumn('train_second', ((col('group_size') - col('test')) * 0.5).cast('int'))\\\n",
    "                    .withColumn('train_first', (col('group_size') - col('train_second') - col('test')))\\\n",
    "                    .withColumn('train_second_index', col('train_first') + col('train_second'))\\\n",
    "                    .withColumn('test_index', col('train_second_index') + col('test'))\n",
    "    df = df.join(group_sizes, on='user_id', how='left')\n",
    "    \n",
    "    df = df.withColumn('first_dataset', col('group_index') <= col('train_first'))\\\n",
    "            .withColumn('second_dataset', (col('group_index') > col('train_first')) & \\\n",
    "                                            (col('group_index') <= col('train_second_index')))\\\n",
    "            .withColumn('third_dataset', (col('group_index') > col('train_second_index')) & \\\n",
    "                                            (col('group_index') <= col('test_index')))\n",
    "\n",
    "    first_train = df.filter(df['first_dataset'] == True).select(cols)\n",
    "    second_train = df.filter(df['second_dataset'] == True).select(cols)\n",
    "    test = df.filter(df['third_dataset'] == True).select(cols)\n",
    "    return first_train, second_train, test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_als, train_cb, test = train_test_split(rating_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Split testing\n",
    "#Проверка, все ли ок разбилось\n",
    "\n",
    "F = rating_df.groupBy('user_id').count().select('user_id', col('count').alias('group_cnt'))\n",
    "A = train_als.groupBy('user_id').count().select('user_id', col('count').alias('als_cnt'))\n",
    "B = train_cb.groupBy('user_id').count().select('user_id', col('count').alias('cb_cnt'))\n",
    "C = test.groupBy('user_id').count().select('user_id', col('count').alias('test_cnt'))\n",
    "D = A.join(B, on='user_id', how='left').join(C, on='user_id', how='left').join(F, on='user_id', how='left')\n",
    "D.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump train_als/train_cb/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(df, path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        df.write.parquet(path)\n",
    "    else:\n",
    "        df.write.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet(train_als, \"../data/train_als\")\n",
    "write_parquet(train_cb, \"../data/train_cb\")\n",
    "write_parquet(test, \"../data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First level model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_als = spark.read.parquet(\"../data/train_als\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rate\",\n",
    "          coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "\n",
    "model = als.fit(train_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(IntegerType()))\n",
    "def get_film_ids(arr):\n",
    "    \"\"\"\n",
    "    Функция для извлечения id фильмов после предикта ALS\n",
    "    \"\"\"\n",
    "    return [x[0] for x in arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on boosters\n",
    "\n",
    "Для тестирования ALS на Boosters.pro"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(test_path, \"r\") as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "cSchema = StructType([StructField('user_id', IntegerType(), False)])\n",
    "\n",
    "test_users = list(map(lambda x: [x], test['users']))\n",
    "\n",
    "test_df = spark.createDataFrame(test_users, schema=cSchema)\n",
    "\n",
    "ans = model.recommendForUserSubset(test_df, 20)\n",
    "\n",
    "ans = ans.select(col('user_id').cast(StringType()).alias('user_id'),\n",
    "                 get_film_ids(col('recommendations')).alias('reccomendations'))\n",
    "\n",
    "ans_df = ans.toPandas()\n",
    "\n",
    "result = {}\n",
    "\n",
    "for i in range(ans_df.shape[0]):\n",
    "    result[ans_df.loc[i, 'user_id']] = ans_df.loc[i, 'reccomendations']\n",
    "\n",
    "ans_df.index = ans_df.user_id\n",
    "\n",
    "a = ans_df.reccomendations.to_json(orient = 'index', force_ascii=False)\n",
    "\n",
    "with open('../data/StepDananswerWithTrimmedFeatures.json', \"w\") as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = model.recommendForAllUsers(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 969 µs, sys: 1.36 ms, total: 2.33 ms\n",
      "Wall time: 5.57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rec_df = rec_df.repartition(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 ms, sys: 7.25 ms, total: 10.2 ms\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction_path = '../data/first_level_output_prq'\n",
    "write_parquet(rec_df, prediction_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
