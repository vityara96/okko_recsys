{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField, StringType\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data_path = \"../data/transactions.csv\"\n",
    "catalogue_path = \"../data/catalogue.json\"\n",
    "test_path = \"../data/test_users.json\"\n",
    "ratings_path = \"../data/StepDan_ratings_prq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName(\"OkkoRecSystem\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(ratings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCol = 'user_uid'\n",
    "itemCol = 'element_uid'\n",
    "ratingCol = 'rate'\n",
    "\n",
    "\n",
    "df = df.withColumnRenamed(itemCol, 'item_id')\\\n",
    "        .withColumnRenamed(ratingCol, 'rate')\\\n",
    "        .withColumnRenamed(userCol, 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rating_df = rating_df.withColumn(\"rate\", col(\"rate\").cast('int'))\\\n",
    "                    .withColumn(\"user_id\", col(\"user_id\").cast('int'))\\\n",
    "                    .withColumn(\"item_id\", col(\"item_id\").cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.withColumn(\"user_id\", col(\"user_id\").cast('int'))\\\n",
    "                    .withColumn(\"item_id\", col(\"item_id\").cast('int'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "rate = {\n",
    "    \"R\": '1',\n",
    "    \"S\": '1',\n",
    "    \"P\": '1'\n",
    "}\n",
    "\n",
    "rating_df = spark.sql(\"SELECT user_uid AS user_id, element_uid AS item_id, ts, consumption_mode AS rate FROM df\")\n",
    "\n",
    "rating_df = rating_df.replace(rate, subset=['rate'])\\\n",
    "                    .withColumn(\"rate\", col(\"rate\").cast('int'))\\\n",
    "                    .withColumn(\"user_id\", col(\"user_id\").cast('int'))\\\n",
    "                    .withColumn(\"item_id\", col(\"item_id\").cast('int'))\n",
    "\n",
    "#sampling\n",
    "rating_df = rating_df.sample(withReplacement=False, fraction=0.02, seed=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_cnt = rating_df.groupBy('user_id').count()\\\n",
    "            .withColumn('enough_films', col('count') >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.join(film_cnt, on='user_id', how='left')\\\n",
    "            .where(col('enough_films') == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOT split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def time_series_split(df, time_col, train_size=0.5):\n",
    "    \n",
    "    df = df.orderBy(time_col, ascending=True)\n",
    "    df_size = df.count()\n",
    "    train_size = int(df_size * train_size)\n",
    "    test_size = df_size - train_size \n",
    "    print(train_size, test_size)\n",
    "    train = df.limit(train_size)\n",
    "    df = df.orderBy(time_col, ascending=False)\n",
    "    test = df.limit(test_size)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_als, train_cb = time_series_split(rating_df, 'ts')\n",
    "\n",
    "train_cb.show()\n",
    "\n",
    "train, test = time_series_split(train_cb, 'ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomSplit "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rating_df = rating_df.drop(\"ts\").limit(10000)\n",
    "\n",
    "train_als, train_cb, test = rating_df.randomSplit([0.4, 0.4, 0.2])\n",
    "\n",
    "train_als.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client oriented split on train/train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_cols = ['user_id', 'item_id', 'ts', 'rate']\n",
    "\n",
    "rating_df = rating_df.select(fin_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.2):\n",
    "    \n",
    "    cols = df.columns\n",
    "    \n",
    "    window = Window.orderBy(\"ts\")\\\n",
    "                .partitionBy('user_id')\\\n",
    "                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    df = df.withColumn('group_index', row_number().over(window))\n",
    "    \n",
    "    group_sizes = rating_df.groupBy('user_id').count()\\\n",
    "                    .select('user_id', col('count').alias('group_size'))\\\n",
    "                    .withColumn('test', ceil(col('group_size') * test_size))\\\n",
    "                    .withColumn('train_second', ((col('group_size') - col('test')) * 0.5).cast('int'))\\\n",
    "                    .withColumn('train_first', (col('group_size') - col('train_second') - col('test')))\\\n",
    "                    .withColumn('train_second_index', col('train_first') + col('train_second'))\\\n",
    "                    .withColumn('test_index', col('train_second_index') + col('test'))\n",
    "    df = df.join(group_sizes, on='user_id', how='left')\n",
    "    \n",
    "    df = df.withColumn('first_dataset', col('group_index') <= col('train_first'))\\\n",
    "            .withColumn('second_dataset', (col('group_index') > col('train_first')) & \\\n",
    "                                            (col('group_index') <= col('train_second_index')))\\\n",
    "            .withColumn('third_dataset', (col('group_index') > col('train_second_index')) & \\\n",
    "                                            (col('group_index') <= col('test_index')))\n",
    "\n",
    "    first_train = df.filter(df['first_dataset'] == True).select(cols)\n",
    "    second_train = df.filter(df['second_dataset'] == True).select(cols)\n",
    "    test = df.filter(df['third_dataset'] == True).select(cols)\n",
    "    return first_train, second_train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_als, train_cb, test = train_test_split(rating_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Split testing\n",
    "F = rating_df.groupBy('user_id').count().select('user_id', col('count').alias('group_cnt'))\n",
    "A = train_als.groupBy('user_id').count().select('user_id', col('count').alias('als_cnt'))\n",
    "B = train_cb.groupBy('user_id').count().select('user_id', col('count').alias('cb_cnt'))\n",
    "C = test.groupBy('user_id').count().select('user_id', col('count').alias('test_cnt'))\n",
    "D = A.join(B, on='user_id', how='left').join(C, on='user_id', how='left').join(F, on='user_id', how='left')\n",
    "D.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First level model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+----+-----+------------+\n",
      "|user_id|item_id|                ts|rate|count|enough_films|\n",
      "+-------+-------+------------------+----+-----+------------+\n",
      "| 170501|   1570| 42957432.60121477|null|    3|        true|\n",
      "| 170501|    524|42158500.037479214|null|    3|        true|\n",
      "| 170501|   3057|  42226336.1775519|null|    3|        true|\n",
      "|  50891|   5266| 42210037.78274297|null|    4|        true|\n",
      "|  50891|   2174|44274120.350332566|null|    4|        true|\n",
      "|  50891|    439| 42209787.20296109|null|    4|        true|\n",
      "|  50891|   3866|  42208715.3444934|null|    4|        true|\n",
      "| 514373|   3101| 42227380.03900147|null|    5|        true|\n",
      "| 514373|   8987| 42226967.27363485|null|    5|        true|\n",
      "| 514373|    327| 42584781.91646723|null|    5|        true|\n",
      "| 514373|   3370| 42576591.71197143|null|    5|        true|\n",
      "| 514373|   2598| 42594735.64953747|null|    5|        true|\n",
      "| 133052|   1653|  42944692.2293622|null|    3|        true|\n",
      "| 133052|   7767| 42829599.95870013|null|    3|        true|\n",
      "| 133052|   8674| 42626133.71044915|null|    3|        true|\n",
      "| 154017|   5416| 43000677.17493592|null|    4|        true|\n",
      "| 154017|   2714| 42946217.91421577|null|    4|        true|\n",
      "| 154017|   3995|  43000640.8237847|null|    4|        true|\n",
      "| 154017|   5201|  42608735.8927115|null|    4|        true|\n",
      "| 414117|   4369| 44001545.24644667|null|    3|        true|\n",
      "+-------+-------+------------------+----+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_df.where(rating_df['rate'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rate\",\n",
    "          coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "\n",
    "model = als.fit(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(IntegerType()))\n",
    "def get_film_ids(arr):\n",
    "    return [x[0] for x in arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on boosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_path, \"r\") as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "cSchema = StructType([StructField('user_id', IntegerType(), False)])\n",
    "\n",
    "test_users = list(map(lambda x: [x], test['users']))\n",
    "\n",
    "test_df = spark.createDataFrame(test_users, schema=cSchema)\n",
    "\n",
    "ans = model.recommendForUserSubset(test_df, 20)\n",
    "\n",
    "ans = ans.select(col('user_id').cast(StringType()).alias('user_id'),\n",
    "                 get_film_ids(col('recommendations')).alias('reccomendations'))\n",
    "\n",
    "ans_df = ans.toPandas()\n",
    "\n",
    "result = {}\n",
    "\n",
    "for i in range(ans_df.shape[0]):\n",
    "    result[ans_df.loc[i, 'user_id']] = ans_df.loc[i, 'reccomendations']\n",
    "\n",
    "ans_df.index = ans_df.user_id\n",
    "\n",
    "a = ans_df.reccomendations.to_json(orient = 'index', force_ascii=False)\n",
    "\n",
    "with open('../data/answerStepDan3.json', \"w\") as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies = train_als.select('item_id').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = train_als.select('user_id').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = model.recommendForAllUsers(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rec_df = rec_df.repartition(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction_path = '../data/predictions_prq'\n",
    "\n",
    "if os.path.exists(prediction_path):\n",
    "    shutil.rmtree(prediction_path)\n",
    "    rec_df.write.parquet(prediction_path)\n",
    "else:\n",
    "    rec_df.write.parquet(prediction_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = spark.read.parquet('../data/predictions_prq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.withColumn(\"rec_films\", get_film_ids(col('recommendations')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_watched_films = train_als.groupBy('user_id').agg(collect_list('item_id').alias('watched_films'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.join(user_watched_films, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.withColumn('new_films', array_except('rec_films', 'watched_films'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_future_films = train_cb.groupBy('user_id').agg(collect_list('item_id').alias('future_films'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.join(user_future_films, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_intersect = pred_df.select('user_id', array_intersect('new_films', 'future_films').alias('rec_intersection'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def NotEmpty(x):\n",
    "    if x == None:\n",
    "        return 0\n",
    "    elif len(x) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_intersect = rec_intersect.withColumn('intersection_len', NotEmpty('rec_intersection'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pred_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_intersect.groupBy('intersection_len').count().withColumn('count', col('count')/s * 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pred_df.select('user_id',\n",
    "                       array_intersect('new_films', 'future_films').alias('positives'),\n",
    "                      array_except('new_films', 'future_films').alias('negatives'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_negatives = train.select('user_id', explode(col('negatives')).alias('item_id'), lit(0).alias('target'))\n",
    "\n",
    "train_positives = train.select('user_id', explode(col('positives')).alias('item_id'), lit(1).alias('target'))\n",
    "\n",
    "train = train_positives.unionAll(train_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
